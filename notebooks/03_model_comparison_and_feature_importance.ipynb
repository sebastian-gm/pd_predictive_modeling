{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf8aa0d-7abb-42c6-8cfd-2412458b7ece",
   "metadata": {},
   "source": [
    "# Model Comparison and Feature Importance\n",
    "\n",
    "In this notebook, I will:\n",
    "1. Compare multiple machine learning algorithms beyond Logistic Regression, such as Random Forest and XGBoost, to see if I can improve predictive performance.\n",
    "2. Implement basic hyperparameter tuning using GridSearchCV or RandomizedSearchCV to optimize model settings.\n",
    "3. Evaluate models using metrics that are crucial for credit risk analysis, such as AUC-ROC and Precision-Recall curves, since identifying \"bad\" credit risks is more important than overall accuracy.\n",
    "4. Explore feature importance and model interpretability. Understanding which features drive the model’s decisions is vital in a credit risk context, where explainability is often a regulatory and business requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5d5cac-4036-4c3b-bc3e-fb475bcfb1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, RocCurveDisplay, PrecisionRecallDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load the data\n",
    "X_train = joblib.load(\"../data/X_train.pkl\")\n",
    "X_test = joblib.load(\"../data/X_test.pkl\")\n",
    "y_train = joblib.load(\"../data/y_train.pkl\")\n",
    "y_test = joblib.load(\"../data/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d36f6-8e4b-4d07-9902-08642d67e070",
   "metadata": {},
   "source": [
    "### Random Forest Baseline\n",
    "\n",
    "I'll train a basic Random Forest with default parameters and compare its performance to the Logistic Regression baseline. Random Forests are often good at handling complex interactions between features and may provide better recall for the \"bad\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "634a00aa-6a52-43dc-9507-7241bfc85764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.745\n",
      "Random Forest AUC: 0.7909604519774011\n",
      "\n",
      "Classificaction Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.91      0.83       141\n",
      "           1       0.62      0.34      0.44        59\n",
      "\n",
      "    accuracy                           0.74       200\n",
      "   macro avg       0.70      0.63      0.64       200\n",
      "weighted avg       0.73      0.74      0.72       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "auc_rf = roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]) # Calculate AUC\n",
    "print(\"Random Forest Accuracy:\", acc_rf)\n",
    "print(\"Random Forest AUC:\", auc_rf)\n",
    "\n",
    "print(\"\\nClassificaction Report:\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4152a1ce-0127-48d2-94a7-c4890c196a8d",
   "metadata": {},
   "source": [
    "The Random Forest's accuracy and AUC provide a quick snapshot. If the AUC is higher than Logistic Regression’s AUC, it suggests the Random Forest may be better at ranking which customers are more likely to be bad credit risks. I will pay special attention to the recall for the bad class and the AUC-ROC, as these metrics align better with credit risk priorities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33728e-aaa1-4db2-9f0b-7e661e7261e8",
   "metadata": {},
   "source": [
    "#### Model Interpretation\n",
    "\n",
    "**Random Forest (Baseline):**  \n",
    "- **Accuracy:** 0.745  \n",
    "- **AUC:** 0.79  \n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "The baseline Random Forest model gives the following results:\n",
    "\n",
    "- **Accuracy (74.5%)**: Slightly lower than the Logistic Regression models, suggesting it is less effective at overall classification.\n",
    "- **Class 0 (Good Credit)**:\n",
    "  - **Recall (0.91)**: Very high, indicating the model identifies most good credit customers correctly.\n",
    "  - **Precision (0.77)**: Slightly lower than Logistic Regression.\n",
    "- **Class 1 (Bad Credit)**:\n",
    "  - **Recall (0.34)**: Much lower than Logistic Regression, meaning the model misses most bad credit customers.\n",
    "  - **Precision (0.62)**: Lower than Logistic Regression, indicating it is less reliable when predicting bad credit.\n",
    "\n",
    "**Key Takeaways:**\n",
    "While the Random Forest model excels at identifying good credit customers, its recall for the bad credit class is significantly worse than Logistic Regression, which makes it less suitable for a credit risk context without further tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb39d159-fc2f-481d-b255-a2a9f7cbdccb",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Random Forest\n",
    "\n",
    "I will use GridSearchCV to search for optimal hyperparameters for the Random Forest. This process demonstrates how I can improve the model further. I'll tune parameters like `n_estimators`, `max_depth`, and `min_samples_leaf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4d8289-6294-4ede-b845-7cea662d9d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'max_depth': 10, 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "Best Score (AUC): 0.7623129915833197\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = RandomForestClassifier(random_state=42),\n",
    "    param_grid = param_grid,\n",
    "    scoring = 'roc_auc', # Using AUC as the scoring metric\n",
    "    cv = 3,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid_search.best_params_)\n",
    "print(\"Best Score (AUC):\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca97333-4325-46c1-a992-aa47f23164b7",
   "metadata": {},
   "source": [
    "**Interpreting the Grid Search Results:**\n",
    "\n",
    "- `n_estimators`: Number of trees in the forest. More trees can improve performance but increase training time.\n",
    "- `max_depth`: Maximum depth of the trees. Deeper trees can model complex relationships but may overfit.\n",
    "- `min_samples_leaf`: Minimum samples per leaf. Increasing this can reduce overfitting.\n",
    "\n",
    "The best parameters are those that give the highest AUC on the validation folds. With these parameters, I'll retrain the model and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af84e6aa-656d-4fe0-8f92-1640f4fd51d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Random Forest Accuracy: 0.75\n",
      "Optimized Random Forest AUC: 0.8015386464719317\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.96      0.84       141\n",
      "           1       0.71      0.25      0.38        59\n",
      "\n",
      "    accuracy                           0.75       200\n",
      "   macro avg       0.73      0.61      0.61       200\n",
      "weighted avg       0.74      0.75      0.71       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_rf = grid_search.best_estimator_ \n",
    "y_pred_best_rf = best_rf.predict(X_test)\n",
    "\n",
    "acc_best_rf = accuracy_score(y_test, y_pred_best_rf)\n",
    "auc_best_rf = roc_auc_score(y_test, best_rf.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(\"Optimized Random Forest Accuracy:\", acc_best_rf)\n",
    "print(\"Optimized Random Forest AUC:\", auc_best_rf)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfabe7d-c492-4d7a-8c87-5e8057ef0fae",
   "metadata": {},
   "source": [
    "#### Model Interpretation\n",
    "\n",
    "**Random Forest (Optimized):**  \n",
    "- **Accuracy:** 0.75  \n",
    "- **AUC:** 0.80154  \n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "After tuning the Random Forest with GridSearchCV, the model's performance is as follows:\n",
    "\n",
    "- **Accuracy (75%)**: Slight improvement over the baseline Random Forest.\n",
    "- **Class 0 (Good Credit)**:\n",
    "  - **Recall (0.96)**: Increased significantly, making it very effective at identifying good credit customers.\n",
    "  - **Precision (0.75)**: Similar to the baseline model.\n",
    "- **Class 1 (Bad Credit)**:\n",
    "  - **Recall (0.25)**: Decreased from the baseline Random Forest, which is concerning.\n",
    "  - **Precision (0.71)**: Improved slightly.\n",
    "\n",
    "**Key Takeaways:**\n",
    "While the optimized Random Forest improves overall metrics like accuracy and AUC, its recall for the bad credit class drops even further, making it less effective for practical use in credit risk management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a5fcd9-f1f9-4ea0-9195-5a127ec47881",
   "metadata": {},
   "source": [
    "### Trying XGBoost\n",
    "\n",
    "XGBoost often performs well in tabular data tasks. I'll train a basic XGBoost classifier and see if it outperforms Random Forest. If it does well, I could also consider tuning its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e53c85e5-8bb7-41e5-807b-353950404536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.785\n",
      "XGBoost AUC: 0.8139199423007573\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       141\n",
      "           1       0.67      0.53      0.59        59\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.75      0.71      0.72       200\n",
      "weighted avg       0.78      0.79      0.78       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SebastianGM\\anaconda3\\envs\\pd_model_env\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [06:30:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "auc_xgb = roc_auc_score(y_test, xgb.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(\"XGBoost Accuracy:\", acc_xgb)\n",
    "print(\"XGBoost AUC:\", auc_xgb)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd966c3d-f47d-4b2c-b778-49cb90c80eec",
   "metadata": {},
   "source": [
    "#### Model Interpretation\n",
    "\n",
    "**XGBoost:**  \n",
    "- **Accuracy:** 0.785  \n",
    "- **AUC:** 0.81392  \n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "XGBoost delivers the following results:\n",
    "\n",
    "- **Accuracy (78.5%)**: Higher than both Logistic Regression and Random Forest models.\n",
    "- **Class 0 (Good Credit)**:\n",
    "  - **Recall (0.89)**: Slightly lower than the optimized Random Forest but still strong.\n",
    "  - **Precision (0.82)**: Better than Random Forest.\n",
    "- **Class 1 (Bad Credit)**:\n",
    "  - **Recall (0.53)**: A significant improvement over both Random Forest models, on par with the improved Logistic Regression.\n",
    "  - **Precision (0.67)**: Matches the improved Logistic Regression.\n",
    "\n",
    "**Key Takeaways:**\n",
    "XGBoost offers a good balance between overall performance and recall for the bad credit class. Its ability to identify more bad credit customers compared to Random Forest makes it a strong candidate for credit risk modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1dc1f9-bbe9-4907-a84b-553e21489347",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Understanding which features matter most is crucial. I'll examine the feature importances from the Random Forest and XGBoost models. This helps stakeholders understand what drives credit risk decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf2472ce-f911-4352-8bb2-a24455679512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Status_of_existing_checking_account_A14</td>\n",
       "      <td>0.137842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Credit_amount</td>\n",
       "      <td>0.129003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Age_in_years</td>\n",
       "      <td>0.100649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Duration_in_month</td>\n",
       "      <td>0.097807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Installment_rate_in_percentage_of_disposable_i...</td>\n",
       "      <td>0.037479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Credit_history_A34</td>\n",
       "      <td>0.036118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Present_residence_since</td>\n",
       "      <td>0.033003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Housing_A152</td>\n",
       "      <td>0.030111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Status_of_existing_checking_account_A12</td>\n",
       "      <td>0.023187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Other_installment_plans_A143</td>\n",
       "      <td>0.022276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              feature  importance\n",
       "9             Status_of_existing_checking_account_A14    0.137842\n",
       "1                                       Credit_amount    0.129003\n",
       "4                                        Age_in_years    0.100649\n",
       "0                                   Duration_in_month    0.097807\n",
       "2   Installment_rate_in_percentage_of_disposable_i...    0.037479\n",
       "13                                 Credit_history_A34    0.036118\n",
       "3                             Present_residence_since    0.033003\n",
       "41                                       Housing_A152    0.030111\n",
       "7             Status_of_existing_checking_account_A12    0.023187\n",
       "40                       Other_installment_plans_A143    0.022276"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = best_rf.feature_importances_\n",
    "feature_names = X_train.columns \n",
    "\n",
    "# Create a DataFrame with feature importances\n",
    "feat_imp_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "feat_imp_df = feat_imp_df.sort_values('importance', ascending=False)\n",
    "\n",
    "feat_imp_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cb9174-8849-4c3d-b8a6-fae6d5cc4f82",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis\n",
    "\n",
    "Understanding which features contribute most to a model's decisions is crucial for interpretability. For my **Random Forest model**, I examined the feature importances, which indicate how much each feature contributes to reducing impurity in the splits.\n",
    "\n",
    "#### Top 10 Most Important Features:\n",
    "1. **No Checking Account (Status_of_existing_checking_account_A14)**: The absence of a checking account appears to be the most influential feature in predicting credit risk.\n",
    "2. **Credit Amount**: The amount of the loan requested is another significant predictor, which aligns with real-world expectations.\n",
    "3. **Age in Years**: Customer age influences creditworthiness, possibly because it correlates with financial stability.\n",
    "4. **Duration in Months**: The length of the credit request period is a key factor.\n",
    "5. **Installment Rate**: The proportion of income used for installment payments also impacts credit risk.\n",
    "\n",
    "By identifying these features, I can better understand the drivers of credit decisions. This insight is valuable for stakeholders and aligns with business requirements for transparency and trust in the model's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a8d38c5-1036-4544-8457-1647c891f4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>No Checking Account</td>\n",
       "      <td>0.137842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Credit Amount</td>\n",
       "      <td>0.129003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.100649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Credit Duration</td>\n",
       "      <td>0.097807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Installment Rate</td>\n",
       "      <td>0.037479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Critical Credit History</td>\n",
       "      <td>0.036118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Years at Residence</td>\n",
       "      <td>0.033003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Own Housing</td>\n",
       "      <td>0.030111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Low Balance Checking</td>\n",
       "      <td>0.023187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>No Other Installment Plans</td>\n",
       "      <td>0.022276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature  importance\n",
       "9          No Checking Account    0.137842\n",
       "1                Credit Amount    0.129003\n",
       "4                          Age    0.100649\n",
       "0              Credit Duration    0.097807\n",
       "2             Installment Rate    0.037479\n",
       "13     Critical Credit History    0.036118\n",
       "3           Years at Residence    0.033003\n",
       "41                 Own Housing    0.030111\n",
       "7         Low Balance Checking    0.023187\n",
       "40  No Other Installment Plans    0.022276"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping original feature names to clean, readable names\n",
    "clean_feature_names = {\n",
    "    \"Status_of_existing_checking_account_A14\": \"No Checking Account\",\n",
    "    \"Credit_amount\": \"Credit Amount\",\n",
    "    \"Age_in_years\": \"Age\",\n",
    "    \"Duration_in_month\": \"Credit Duration\",\n",
    "    \"Installment_rate_in_percentage_of_disposable_income\": \"Installment Rate\",\n",
    "    \"Credit_history_A34\": \"Critical Credit History\",\n",
    "    \"Present_residence_since\": \"Years at Residence\",\n",
    "    \"Housing_A152\": \"Own Housing\",\n",
    "    \"Status_of_existing_checking_account_A12\": \"Low Balance Checking\",\n",
    "    \"Other_installment_plans_A143\": \"No Other Installment Plans\"\n",
    "}\n",
    "\n",
    "# Replace feature names with clean names\n",
    "feat_imp_df['feature'] = feat_imp_df['feature'].replace(clean_feature_names)\n",
    "\n",
    "# Sort and display the top 10 features\n",
    "feat_imp_df = feat_imp_df.sort_values('importance', ascending=False)\n",
    "feat_imp_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b1f068-b524-458d-92c0-541348a52e23",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis (Cleaned)\n",
    "\n",
    "The table below highlights the top 10 most important features from the **Random Forest model**. These features contribute most to the model's decision-making process by reducing impurity at splits:\n",
    "\n",
    "| Feature                     | Importance |\n",
    "|-----------------------------|------------|\n",
    "| No Checking Account         | 0.137842   |\n",
    "| Credit Amount               | 0.129003   |\n",
    "| Age                         | 0.100649   |\n",
    "| Credit Duration             | 0.097807   |\n",
    "| Installment Rate            | 0.037479   |\n",
    "| Critical Credit History     | 0.036118   |\n",
    "| Years at Residence          | 0.033003   |\n",
    "| Own Housing                 | 0.030111   |\n",
    "| Low Balance Checking        | 0.023187   |\n",
    "| No Other Installment Plans  | 0.022276   |\n",
    "\n",
    "These results provide insight into the factors driving credit risk predictions. For example:\n",
    "- Customers without a checking account or with higher loan amounts are more likely to be flagged as risky.\n",
    "- Features like age and credit duration also play a significant role, indicating that customer demographics and loan terms heavily influence decisions.\n",
    "\n",
    "This step ensures that the model aligns with business expectations and highlights which features are most relevant for decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5817c96-aff9-4f98-81ad-e422a7e980e2",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis (XGBoost)\n",
    "\n",
    "After analyzing the feature importances for the **Random Forest model**, I will now examine the importances for the **XGBoost model**. Understanding how different models prioritize features provides deeper insights into the dataset and helps identify consistent patterns.\n",
    "\n",
    "#### Why Analyze XGBoost Feature Importances?\n",
    "XGBoost uses gradient boosting, a different approach than Random Forest. As a result, the way it prioritizes features can differ. Comparing feature importances across models helps:\n",
    "- Validate whether key features are consistent across models.\n",
    "- Identify any unique features emphasized by XGBoost.\n",
    "\n",
    "In this step, I will:\n",
    "1. Extract feature importances from the XGBoost model.\n",
    "2. Map feature names to clean, readable names for clarity.\n",
    "3. Display the top 10 most important features.\n",
    "4. Visualize the results for better interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eba93a13-a223-47f7-b08a-d8d268f79e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>No Checking Account</td>\n",
       "      <td>0.102722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Other_debtors_or_guarantors_A103</td>\n",
       "      <td>0.067574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Status_of_existing_checking_account_A13</td>\n",
       "      <td>0.038816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Savings_account_bonds_A64</td>\n",
       "      <td>0.032750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Property_A124</td>\n",
       "      <td>0.031116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Savings_account_bonds_A65</td>\n",
       "      <td>0.030579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Purpose_A45</td>\n",
       "      <td>0.029664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Critical Credit History</td>\n",
       "      <td>0.028660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Present_employment_since_A74</td>\n",
       "      <td>0.027591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>No Other Installment Plans</td>\n",
       "      <td>0.026665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    feature  importance\n",
       "9                       No Checking Account    0.102722\n",
       "35         Other_debtors_or_guarantors_A103    0.067574\n",
       "8   Status_of_existing_checking_account_A13    0.038816\n",
       "25                Savings_account_bonds_A64    0.032750\n",
       "38                            Property_A124    0.031116\n",
       "26                Savings_account_bonds_A65    0.030579\n",
       "19                              Purpose_A45    0.029664\n",
       "13                  Critical Credit History    0.028660\n",
       "29             Present_employment_since_A74    0.027591\n",
       "40               No Other Installment Plans    0.026665"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost Feature \n",
    "xgb_importances = xgb.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame with feature importances\n",
    "xgb_feat_imp_df = pd.DataFrame({'feature': feature_names, 'importance': xgb_importances})\n",
    "\n",
    "# Replace feature names with clean names\n",
    "xgb_feat_imp_df['feature'] = xgb_feat_imp_df['feature'].replace(clean_feature_names)\n",
    "\n",
    "# Sort and display the top 10 features\n",
    "xgb_feat_imp_df = xgb_feat_imp_df.sort_values('importance', ascending=False)\n",
    "xgb_feat_imp_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c51887-44d6-4546-9d8c-4ade3e120f0a",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis (XGBoost)\n",
    "\n",
    "#### Key Observations:\n",
    "1. **No Checking Account**: Similar to the Random Forest model, this feature remains the most significant predictor of credit risk. Customers without a checking account are flagged as higher risk.\n",
    "2. **Guarantors**: Having a guarantor (or lack thereof) also plays a key role in distinguishing risk levels, as highlighted by the `Other_debtors_or_guarantors_A103` feature.\n",
    "3. **Savings and Property**: Features related to savings accounts (`Savings A64`, `Savings A65`) and property ownership (`Property A124`) significantly influence the model's predictions. Customers without savings or property are seen as higher risk.\n",
    "4. **Purpose of Credit**: The model also identifies the credit purpose (`Repairs Purpose`) as an important factor, reflecting the potential risk associated with specific types of loans.\n",
    "5. **Employment Stability**: Employment duration (`Employment: 4-7 years`) is another important feature, indicating that longer employment histories may correlate with lower credit risk.\n",
    "\n",
    "#### Comparison to Random Forest:\n",
    "- Both models prioritize **No Checking Account** as the most important feature, reinforcing its influence on credit risk predictions.\n",
    "- XGBoost places more emphasis on guarantors and savings-related features, while Random Forest highlights loan duration and age.\n",
    "- These differences may reflect the unique strengths of each model in interpreting the dataset.\n",
    "\n",
    "### Why Feature Importance Matters in Credit Risk\n",
    "\n",
    "Identifying the most important features in credit risk analysis is crucial for both stakeholders and regulators. These insights can:\n",
    "1. Help financial institutions make more informed lending decisions.\n",
    "2. Highlight key customer attributes that influence creditworthiness.\n",
    "3. Ensure the model aligns with business priorities, such as minimizing risky loans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48f19be-1794-4788-ac3b-92ec61122103",
   "metadata": {},
   "source": [
    "### SHAP: Enhancing Feature Interpretability\n",
    "\n",
    "To complement the feature importance analysis, I used **SHAP (SHapley Additive exPlanations)** to understand how features contribute to individual predictions. SHAP values allow me to:\n",
    "1. Explain **global importance**: Which features matter most across all predictions.\n",
    "2. Explain **local importance**: How specific features influence individual predictions.\n",
    "\n",
    "This deeper interpretability is particularly valuable in credit risk contexts, where regulatory requirements often demand transparency and explainability in decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b630f29-b7ed-4105-a424-da60f742639b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot cast array data from dtype('O') to dtype('float64') according to the rule 'safe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create an explainer for the XGBoost model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate SHAP values for the test set\u001b[39;00m\n\u001b[0;32m      7\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pd_model_env\\lib\\site-packages\\shap\\explainers\\_explainer.py:188\u001b[0m, in \u001b[0;36mExplainer.__init__\u001b[1;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m=\u001b[39m explainers\u001b[38;5;241m.\u001b[39mTreeExplainer\n\u001b[1;32m--> 188\u001b[0m     explainers\u001b[38;5;241m.\u001b[39mTreeExplainer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker, link\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlink, feature_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, linearize_link\u001b[38;5;241m=\u001b[39mlinearize_link, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditive\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m=\u001b[39m explainers\u001b[38;5;241m.\u001b[39mAdditiveExplainer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pd_model_env\\lib\\site-packages\\shap\\explainers\\_tree.py:195\u001b[0m, in \u001b[0;36mTreeExplainer.__init__\u001b[1;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, link, linearize_link)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_perturbation \u001b[38;5;241m=\u001b[39m feature_perturbation\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mTreeEnsemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_missing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output \u001b[38;5;241m=\u001b[39m model_output\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pd_model_env\\lib\\site-packages\\shap\\explainers\\_tree.py:1046\u001b[0m, in \u001b[0;36mTreeEnsemble.__init__\u001b[1;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_booster()\n\u001b[1;32m-> 1046\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_xgboost_model_attributes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective_name_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtree_output_name_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_stacked_models \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1055\u001b[0m         \u001b[38;5;66;03m# with predict_proba we need to double the outputs to match\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pd_model_env\\lib\\site-packages\\shap\\explainers\\_tree.py:1280\u001b[0m, in \u001b[0;36mTreeEnsemble._set_xgboost_model_attributes\u001b[1;34m(self, data, data_missing, objective_name_map, tree_output_name_map)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1278\u001b[0m loader \u001b[38;5;241m=\u001b[39m XGBTreeModelLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_model)\n\u001b[1;32m-> 1280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trees\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_missing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_offset \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mbase_score\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m objective_name_map\u001b[38;5;241m.\u001b[39mget(loader\u001b[38;5;241m.\u001b[39mname_obj, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pd_model_env\\lib\\site-packages\\shap\\explainers\\_tree.py:1996\u001b[0m, in \u001b[0;36mXGBTreeModelLoader.get_trees\u001b[1;34m(self, data, data_missing)\u001b[0m\n\u001b[0;32m   1986\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_trees):\n\u001b[0;32m   1987\u001b[0m     info \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1988\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchildren_left\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_cleft[i],\n\u001b[0;32m   1989\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchildren_right\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_cright[i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1994\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_sample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_hess[i],\n\u001b[0;32m   1995\u001b[0m     }\n\u001b[1;32m-> 1996\u001b[0m     trees\u001b[38;5;241m.\u001b[39mappend(\u001b[43mSingleTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_missing\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1997\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trees\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pd_model_env\\lib\\site-packages\\shap\\explainers\\_tree.py:1723\u001b[0m, in \u001b[0;36mSingleTree.__init__\u001b[1;34m(self, tree, normalize, scaling, data, data_missing)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m data_missing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_sample_weight\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m-> 1723\u001b[0m     \u001b[43m_cext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_tree_update_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren_default\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_missing\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1728\u001b[0m \u001b[38;5;66;03m# we compute the expectations to make sure they follow the SHAP logic\u001b[39;00m\n\u001b[0;32m   1729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth \u001b[38;5;241m=\u001b[39m _cext\u001b[38;5;241m.\u001b[39mcompute_expectations(\n\u001b[0;32m   1730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren_left, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren_right, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_sample_weight,\n\u001b[0;32m   1731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m   1732\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot cast array data from dtype('O') to dtype('float64') according to the rule 'safe'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "# Create an explainer for the XGBoost model\n",
    "explainer = shap.Explainer(xgb, X_train)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Visualize the overall feature importance with SHAP\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c30c4d67-2d90-4a91-b97b-b7293b2e8abc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check the data types of your training data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mdtypes)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Check for non-numeric columns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m non_numeric_columns \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfff55c-7d8b-4fd7-8797-74a4b8e2f11d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PD Credit Env",
   "language": "python",
   "name": "pd_model_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
